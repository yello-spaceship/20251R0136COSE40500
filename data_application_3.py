# -*- coding: utf-8 -*-
"""데과응과제3.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1aW5uB0BbwFdDWf7-GrWQ_dHCI1Qf0B1E

# 라이브러리/ 불러오기
"""

import pandas as pd
from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import train_test_split, cross_val_score
from sklearn.linear_model import LinearRegression
from sklearn.tree import DecisionTreeRegressor
from sklearn.ensemble import RandomForestRegressor, AdaBoostRegressor, GradientBoostingRegressor
from sklearn.metrics import mean_squared_error
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.pipeline import Pipeline
from sklearn.compose import ColumnTransformer
from sklearn.tree import DecisionTreeRegressor
from sklearn.ensemble import GradientBoostingRegressor
from sklearn.metrics import mean_squared_error
from sklearn.preprocessing import OneHotEncoder, StandardScaler

import pandas as pd

# 엑셀 파일을 읽어오기
excel_file_path = 'question3-test - features.xlsx'
data = pd.read_excel(excel_file_path)

# CSV 파일로 저장하기
csv_file_path = 'question3-test - features.csv'
data.to_csv(csv_file_path, index=False)

data=pd.read_csv('question3-test - features.csv',encoding='ISO-8859-1')
#feature
X=data.drop(['Federation','MeetCountry','MeetTown','MeetName'],axis=1)
non_nan_counts = X.notna().sum()

#print("각 컬럼에서 결측치가 아닌 값의 개수:")
#print(non_nan_counts)



"""# 전처리-점수 및 성별"""

import numpy as np
from sklearn.compose import ColumnTransformer
from sklearn.preprocessing import StandardScaler, OneHotEncoder
from sklearn.pipeline import Pipeline
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_absolute_error
import tensorflow as tf
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.callbacks import EarlyStopping
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.optimizers.schedules import ExponentialDecay

import pandas as pd
import numpy as np
from sklearn.ensemble import GradientBoostingRegressor
from sklearn.preprocessing import OneHotEncoder
from sklearn.compose import ColumnTransformer


# 컬럼 타입 변환 및 NaN 처리
cols = ['Dots', 'Wilks', 'Glossbrenner', 'Goodlift', 'Best3BenchKg','Bench1Kg','Bench2Kg','Bench3Kg']
data[cols] = data[cols].apply(pd.to_numeric, errors='coerce').fillna(0).astype(float)


# NaN이 아닌 값의 개수를 고려한 Score 계산
data['Score'] = data[['Dots', 'Wilks', 'Glossbrenner', 'Goodlift']].apply(
    lambda row: row.sum() / row.count() if row.count() > 0 else 0,
    axis=1
)

# 'Sex' 열에서 'F'는 1, 'M'은 0으로 변환하고, 결측치는 0으로 채움
# 'Sex' 열에서 'F'는 1, 'M'은 0으로 변환하고, 결측치는 0으로 채움
data['Sex'] = np.where(data['Sex'] == 'F', 1, 0)

# 매핑 딕셔너리 정의
event_mapping = {
    'S': 1,
    'B': 2,
    'D': 3,
    'SB': 4,
    'SD': 5,
    'BD': 6,
    'SBD': 7
}

# Event 열에 매핑 적용
data['Event'] = data['Event'].map(event_mapping)

count_sex_f = (data['Sex'] == 1).sum()
count_sex_m = (data['Sex'] == 0).sum()
print("Sex='F' 개수:", count_sex_f)
print("Sex='M' 개수:", count_sex_m)

data.head()

"""# 나이 채우기&도구"""

# AgeClass 중앙값 계산 함수
def get_age_class_median(age_class):
    if pd.isna(age_class):
        return np.nan
    try:
        age_range = age_class.split('-')
        lower_bound = int(age_range[0])
        upper_bound = int(age_range[1]) if age_range[1].isdigit() else 999
        return (lower_bound + upper_bound) / 2
    except:
        return np.nan

# AgeClass의 중앙값을 계산하여 Age 열에 채움
age_class_medians = data['AgeClass'].apply(get_age_class_median)
data['Age'] = data['Age'].combine_first(age_class_medians)



# BirthYearClass 중앙값 계산 함수
def get_birth_year_class_median(birth_year_class):
    if pd.isna(birth_year_class):
        return np.nan
    try:
        year_range = birth_year_class.split('-')
        lower_bound = int(year_range[0])
        upper_bound = int(year_range[1]) if year_range[1].isdigit() else 999
        return (lower_bound + upper_bound) / 2
    except:
        return np.nan

# BirthYearClass 중앙값을 계산하여 Age 열에 채움
birth_year_class_medians = data['BirthYearClass'].apply(get_birth_year_class_median)
data['Age'] = data['Age'].combine_first(birth_year_class_medians)

# 특정 조건에 따른 Age 값 채우기
data.loc[data['Age'].isna() & data['BirthYearClass'].str.contains('Junior|School|Student', na=False), 'Age'] = 18
data.loc[data['Age'].isna() & (data['BirthYearClass'] == '70-999'), 'Age'] = 75
data['Age'] = pd.to_numeric(data['Age'], errors='coerce')

# Age 열의 NaN 처리 (Score를 사용하여 예측)
train_data = data[data['Age'].notna()]
test_data = data[data['Age'].isna()]

# Age 예측을 위한 GradientBoostingRegressor 모델
if not test_data.empty:
    age_regressor = GradientBoostingRegressor(random_state=42)
    age_regressor.fit(train_data[['Score']], train_data['Age'])
    predicted_ages = age_regressor.predict(test_data[['Score']])
    data.loc[data['Age'].isna(), 'Age'] = predicted_ages

data['Equipment'] = data['Equipment'].apply(lambda x:
                                                    0 if x == 'Raw' else
                                                    1 if x == 'Wraps' else
                                                    2 if x == 'Single-Ply' else
                                                    3 if x == 'Multi-Ply' else
                                                    4 if x == 'Unlimited' else
                                                    5)
data['Equipment'].fillna(0, inplace=True)

import pandas as pd

# 'Place' 컬럼을 숫자로 변환하면서 숫자가 아닌 값은 NaN으로 처리
data['Place'] = pd.to_numeric(data['Place'], errors='coerce')

# NaN 값을 0으로 채움
data['Place'].fillna(0, inplace=True)
data['Place']=data['Place'].astype(int)
data['Place'].fillna(0, inplace=True)

data.head()

data.head()

"""# 데이터 전처리3"""

data=data.drop(['Federation','MeetCountry','MeetTown','MeetName','Sanctioned','BirthYearClass','AgeClass'],axis=1)

# 조건에 따라 Best3BenchKg 값을 채워 넣기
# Bench1Kg, Bench2Kg, Bench3Kg 컬럼을 float 타입으로 변환
data['Bench1Kg'] = data['Bench1Kg'].astype(float)
data['Bench2Kg'] = data['Bench2Kg'].astype(float)
data['Bench3Kg'] = data['Bench3Kg'].astype(float)

def fill_bench_best(row):
    if pd.isna(row['Best3BenchKg']):
        # Bench1Kg, Bench2Kg, Bench3Kg 중 양수 값이 있는 경우 그 중 하나를 선택
        positive_values = [v for v in [row['Bench1Kg'], row['Bench2Kg'], row['Bench3Kg']] if v > 0]
        if row['Bench1Kg'] > 0 and row['Bench2Kg'] > 0 and row['Bench3Kg'] > 0:
            return max(positive_values)
            return  max(abs(row['Bench1Kg']), abs(row['Bench2Kg']), abs(row['Bench3Kg']))
        else:
            # 모두 음수인 경우 절대값에서 10을 뺀 값으로 채움
            return max(abs(row['Bench1Kg']), abs(row['Bench2Kg']), abs(row['Bench3Kg'])) - 10
    return row['Best3BenchKg']

data['Best3BenchKg'] = data.apply(fill_bench_best, axis=1)

data.head()

# Division 컬럼을 Master, Junior, 나머지로 매핑
def map_division(division):
        if pd.isna(division):
            return 2  # 결측치도 2로 처리
        elif 'Master' in division:
            return 0
        elif 'Junior' in division:
            return 1
        elif 'Pro' in division:
            return 2
        else:
            return 3

data['Division'] = data['Division'].apply(map_division)

data=data.drop(['Bench1Kg','Bench2Kg','Bench3Kg','Dots','Wilks','Glossbrenner','Goodlift'],axis=1)

data['Event'].fillna('0', inplace=True)

data=data.drop(['Event_mapped'],axis=1)

data.count()

# TotalKg 컬럼에서 NaN 값을 0으로 대체
data['TotalKg'] = pd.to_numeric(data['TotalKg'], errors='coerce')
data['TotalKg'].fillna(0, inplace=True)
data['BodyweightKg'] = pd.to_numeric(data['BodyweightKg'], errors='coerce')

if data['BodyweightKg'].isnull().sum() > 0:
    # 예측에 TotalKg을 제외하고 나머지 컬럼만 사용하도록 수정
    features = [col for col in data.columns if col not in ['BodyweightKg', 'TotalKg']]

    X_non_null = data.dropna(subset=['BodyweightKg'])
    X_train = X_non_null[features]
    y_train = X_non_null['BodyweightKg']

    regressor = GradientBoostingRegressor(random_state=42)
    regressor.fit(X_train, y_train)

    X_test = data[data['BodyweightKg'].isnull()][features]
    predicted_bodyweights = regressor.predict(X_test)

    data.loc[data['BodyweightKg'].isnull(), 'BodyweightKg'] = predicted_bodyweights

X=data

X

X.head()

# 변수 지정
categorical = ['Sex', 'Equipment','Division','Place','Event']
numerical = ['Best3BenchKg', 'BodyweightKg', 'Score','Age']
target = 'TotalKg'
x = X[categorical + numerical]
y = X[target]
X_train, X_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=42)

# ColumnTransformer와 파이프라인 정의
x = X[categorical + numerical]
y = X[target]
X_train, X_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=42)

preprocessor = ColumnTransformer(
    transformers=[
        ('cat', OneHotEncoder(handle_unknown='ignore'), categorical)
    ],
    remainder='passthrough'
)



"""# 학습"""

import numpy as np
from sklearn.compose import ColumnTransformer
from sklearn.preprocessing import StandardScaler, OneHotEncoder
from sklearn.pipeline import Pipeline
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_absolute_error
import tensorflow as tf
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.callbacks import EarlyStopping
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.optimizers.schedules import ExponentialDecay

# Ensure target variable y is in the correct format without using .values
y_train = y_train.astype(np.float32)
y_test = y_test.astype(np.float32)
X_train=X_train.astype(np.float32)
X_test=X_test.astype(np.float32)

# Preprocessing steps
X_train_processed = preprocessor.fit_transform(X_train)
X_test_processed = preprocessor.transform(X_test)

# Double-check for missing or unexpected values in preprocessed data
X_train_processed = np.nan_to_num(X_train_processed)
X_test_processed = np.nan_to_num(X_test_processed)
initial_learning_rate = 0.001  # 초기 학습률을 약간 높게 설정

optimizer = Adam(learning_rate=0.001)
# DNN 모델 정의 함수
def create_model(input_shape):
    model = tf.keras.Sequential([
        tf.keras.layers.Dense(64, activation='relu', input_shape=(input_shape,)),
        tf.keras.layers.Dense(32, activation='relu'),
        tf.keras.layers.Dense(16, activation='relu'),
        tf.keras.layers.Dense(1)  # 회귀 모델용 출력 레이어
    ])
    model.compile(optimizer=optimizer, loss='mse', metrics=['mae'])
    return model

# 모델 생성 및 학습
model = create_model(X_train_processed.shape[1])
early_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)
model.fit(X_train_processed, y_train, epochs=20, batch_size=64, validation_split=0.2, verbose=1, callbacks=[early_stopping])

# 모델 평가
y_pred = model.predict(X_test_processed)
test_mae = mean_absolute_error(y_test, y_pred)
print(f'Test MAE: {test_mae}')

import pandas as pd
from tensorflow.keras.models import save_model

# Predict on the test set
y_pred = model.predict(X_test_processed)

# Convert predictions and actual values into a DataFrame for comparison
comparison_df = pd.DataFrame({
    'Actual': y_test,
    'Predicted': y_pred.flatten()  # Flatten to ensure it's a 1D array
})

# Display the first few rows of the comparison DataFrame
print(comparison_df.head())

# Save the model
model.save('data_3.h5')  # Saves in HDF5 format

# Print confirmation of model saving
print("Model saved as 'data_3.h5'")

"""# 히히 예측"""

test_set=pd.read_csv('tmp_data_processed.csv',encoding='ISO-8859-1')

import pandas as pd
tmp=X

# BodyweightKg 열에서 '90+' 값을 100으로 변환
tmp['BodyweightKg'] = tmp['BodyweightKg'].replace('90+', 100)
tmp['BodyweightKg'] = tmp['BodyweightKg'].replace('140+', 150)

tmp.head()

# BodyweightKg 열에서 '90+' 값을 100으로 변환
tmp['BodyweightKg'] = tmp['BodyweightKg'].replace('90+', 100)
tmp['BodyweightKg'] = tmp['BodyweightKg'].replace('140+', 150)

tmp.head()

tmp=tmp.drop(['TotalKg'],axis=1)

"""# 새 섹션"""

# 변수 지정
categorical = ['Sex', 'Equipment','Division','Place']
numerical = ['Best3BenchKg', 'BodyweightKg', 'Score','Age']
# ColumnTransformer와 파이프라인 정의
x = tmp[categorical + numerical]

preprocessor = ColumnTransformer(
    transformers=[
        ('cat', OneHotEncoder(handle_unknown='ignore'), categorical)
    ],
    remainder='passthrough'
)

# 전처리 적용 후 float32로 변환
babo = preprocessor.fit_transform(x).astype('float32')

# 모델 예측
babo_predictions = model.predict(babo)

# 결과 출력
print(babo_predictions)

import pandas as pd
import numpy as np

# 음수 값을 0으로 설정하고 반올림
babo_predictions = np.where(babo_predictions < 0, babo_predictions*(-3), babo_predictions)
babo_predictions = np.round(babo_predictions)  # 반올림

# 예측 결과를 데이터프레임으로 변환
predictions_df = pd.DataFrame(babo_predictions, columns=['PredictedTotalWeight'])

# CSV 파일로 저장
output_predictions_csv = 'bean6.csv'
predictions_df.to_csv(output_predictions_csv, index=False)

print(f"음수 값을 0으로 설정하고 반올림된 예측 결과가 CSV 파일로 저장되었습니다: {output_predictions_csv}")

import pandas as pd

# 예측 결과를 데이터프레임으로 변환
predictions_df = pd.DataFrame(babo_predictions, columns=['PredictedTotalWeight'])

# CSV 파일로 저장
output_predictions_csv = 'bean2.csv'
predictions_df.to_csv(output_predictions_csv, index=False)

print(f"예측 결과가 CSV 파일로 저장되었습니다: {output_predictions_csv}")

import pandas as pd

# 기존 Excel 파일을 CSV로 변환하여 저장
file_path = 'question3-test - features.xlsx'
test_data = pd.read_excel(file_path)

# CSV로 저장
csv_path = 'question3-test-features.csv'
test_data.to_csv(csv_path, index=False)
print(f"CSV 파일로 변환되었습니다: {csv_path}")
temp='predicted_total_weight.csv'

# 예측된 값을 'TotalWeight' 열에 추가 (예: 모델 예측 결과가 babo_predictions에 저장되어 있다고 가정)
test_data['TotalWeight'] = temp

# 수정된 데이터프레임을 새로운 CSV 파일로 저장
output_csv_path = 'neew.csv'
test_data.to_csv(output_csv_path, index=False)
print(f"새로운 CSV 파일이 저장되었습니다: {output_csv_path}")

import pandas as pd
import numpy as np

# 기존 Excel 파일을 CSV로 변환하여 저장
file_path = 'question3-test - features.xlsx'
test_data = pd.read_excel(file_path)

# CSV로 저장
csv_path = 'question3-test-features.csv'
test_data.to_csv(csv_path, index=False)
print(f"CSV 파일로 변환되었습니다: {csv_path}")

# 예측된 값을 'TotalWeight' 열에 추가하고, 음수 값을 0으로 설정
# (예: 모델 예측 결과가 babo_predictions 배열에 저장되어 있다고 가정)
babo_predictions = np.where(temp < 0, 0, temp)  # 음수 값을 0으로 설정

test_data['TotalWeight'] = babo_predictions

# 수정된 데이터프레임을 새로운 CSV 파일로 저장
output_csv_path = 'neew.csv'
test_data.to_csv(output_csv_path, index=False)
print(f"새로운 CSV 파일이 저장되었습니다: {output_csv_path}")

# 행 개수 확인
num_rows = data.shape[0]
print(f"행의 개수는 {num_rows}개 입니다.")
